{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "import h5netcdf\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as sig\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../Data/ds_in.csv', '../Data/ds_inBering.csv', '../Data/ds_inBering30day10.csv', '../Data/ds_inBering30day20.csv', '../Data/ds_inSH.csv', '../Data/ds_inSH30day10.csv', '../Data/ds_inSH30day20.csv', '../Data/ds_inSHWeddell30day10.csv', '../Data/ds_inSHWeddell30day20.csv', '../Data/ds_inWeddell.csv', '../Data/ds_in_30day10.csv', '../Data/ds_in_30day20.csv']\n",
      "['../Data/ds_out.csv', '../Data/ds_outBering.csv', '../Data/ds_outBering30day10.csv', '../Data/ds_outBering30day20.csv', '../Data/ds_outSH.csv', '../Data/ds_outSH30day10.csv', '../Data/ds_outSH30day20.csv', '../Data/ds_outSHWedell30day10.csv', '../Data/ds_outSHWedell30day20.csv', '../Data/ds_outWeddell.csv', '../Data/ds_out_30day10.csv', '../Data/ds_out_30day20.csv']\n"
     ]
    }
   ],
   "source": [
    "file_path='../Data/'\n",
    "data_list_in=glob.glob(file_path+'ds_in*')\n",
    "data_list_out=glob.glob(file_path+'ds_out*')\n",
    "data_list_in.sort()\n",
    "data_list_out.sort()\n",
    "print(data_list_in)\n",
    "print(data_list_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_arrays_in[0]: (45693, 13)\n",
      "Shape of data_arrays_in[1]: (41227, 13)\n",
      "Shape of data_arrays_in[2]: (30775, 13)\n",
      "Shape of data_arrays_in[3]: (56466, 13)\n",
      "Shape of data_arrays_in[4]: (44050, 13)\n",
      "Shape of data_arrays_in[5]: (41869, 13)\n",
      "Shape of data_arrays_in[6]: (3, 13)\n",
      "Shape of data_arrays_in[7]: (16273, 13)\n",
      "Shape of data_arrays_in[8]: (371, 13)\n",
      "Shape of data_arrays_in[9]: (15356, 13)\n",
      "Shape of data_arrays_in[10]: (86622, 13)\n",
      "Shape of data_arrays_in[11]: (71015, 13)\n",
      "Shape of data_arrays_out[0]: (45693, 12)\n",
      "Shape of data_arrays_out[1]: (41227, 12)\n",
      "Shape of data_arrays_out[2]: (30775, 12)\n",
      "Shape of data_arrays_out[3]: (56466, 12)\n",
      "Shape of data_arrays_out[4]: (44050, 12)\n",
      "Shape of data_arrays_out[5]: (41869, 12)\n",
      "Shape of data_arrays_out[6]: (3, 12)\n",
      "Shape of data_arrays_out[7]: (16273, 12)\n",
      "Shape of data_arrays_out[8]: (371, 12)\n",
      "Shape of data_arrays_out[9]: (15356, 12)\n",
      "Shape of data_arrays_out[10]: (86622, 12)\n",
      "Shape of data_arrays_out[11]: (71015, 12)\n"
     ]
    }
   ],
   "source": [
    "data_arrays_in = [np.loadtxt(file, delimiter=',') for file in data_list_in]\n",
    "data_arrays_out = [np.loadtxt(file, delimiter=',') for file in data_list_out]\n",
    "\n",
    "# Print the shapes of the arrays to verify\n",
    "for i, array in enumerate(data_arrays_in):\n",
    "    print(f\"Shape of data_arrays_in[{i}]: {array.shape}\")\n",
    "\n",
    "for i, array in enumerate(data_arrays_out):\n",
    "    print(f\"Shape of data_arrays_out[{i}]: {array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of reshaped_data_arrays_in: (449720, 13)\n"
     ]
    }
   ],
   "source": [
    "# Reshape each array in data_arrays_in to the specified shape and concatenate them\n",
    "reshaped_data_arrays_in = np.concatenate([array.reshape(-1, 13) for array in data_arrays_in])\n",
    "\n",
    "# Print the shape of the reshaped array to verify\n",
    "print(f\"Shape of reshaped_data_arrays_in: {reshaped_data_arrays_in.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of reshaped_data_arrays_out: (449720, 12)\n"
     ]
    }
   ],
   "source": [
    "# Reshape each array in data_arrays_out to the specified shape and concatenate them\n",
    "reshaped_data_arrays_out = np.concatenate([array.reshape(-1, 12) for array in data_arrays_out])\n",
    "\n",
    "# Print the shape of the reshaped array to verify\n",
    "print(f\"Shape of reshaped_data_arrays_out: {reshaped_data_arrays_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cleaned_data_arrays_in: (422480, 13)\n",
      "Shape of cleaned_data_arrays_out: (422480, 12)\n"
     ]
    }
   ],
   "source": [
    "# Clean data_arrays_in and data_arrays_out\n",
    "cleaned_data_arrays_in = []\n",
    "cleaned_data_arrays_out = []\n",
    "\n",
    "for i in range(reshaped_data_arrays_in.shape[0]):\n",
    "        if not np.sum(reshaped_data_arrays_out[i])==0:\n",
    "            cleaned_data_arrays_in.append(reshaped_data_arrays_in[i])\n",
    "            cleaned_data_arrays_out.append(reshaped_data_arrays_out[i])\n",
    "\n",
    "# Convert lists back to numpy arrays if needed\n",
    "cleaned_data_arrays_in = np.array(cleaned_data_arrays_in)\n",
    "cleaned_data_arrays_out = np.array(cleaned_data_arrays_out)\n",
    "\n",
    "# Print the shapes of the cleaned arrays to verify\n",
    "print(f\"Shape of cleaned_data_arrays_in: {cleaned_data_arrays_in.shape}\")\n",
    "print(f\"Shape of cleaned_data_arrays_out: {cleaned_data_arrays_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the orders of magnitude difference\n",
    "def calculate_orders_of_magnitude_difference(predictions, actuals):\n",
    "    differences = torch.abs(predictions - actuals)\n",
    "    orders_of_magnitude_diff = torch.log10(differences + 1e-10) - torch.log10(torch.abs(actuals) + 1e-10)\n",
    "    return torch.mean(torch.abs(orders_of_magnitude_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], MSE Loss: 0.000569 Sum loss: 0.000003 Sign Loss: 0.000000\n",
      "Epoch [200/1000], MSE Loss: 0.000016 Sum loss: 0.000000 Sign Loss: 0.000000\n",
      "Epoch [300/1000], MSE Loss: 0.000013 Sum loss: 0.000000 Sign Loss: 0.000000\n",
      "Epoch [400/1000], MSE Loss: 0.000013 Sum loss: 0.000000 Sign Loss: 0.000000\n",
      "Epoch [500/1000], MSE Loss: 0.000013 Sum loss: 0.000000 Sign Loss: 0.000000\n",
      "Epoch [600/1000], MSE Loss: 0.000012 Sum loss: 0.000000 Sign Loss: 0.000000\n",
      "Epoch [700/1000], MSE Loss: 0.000011 Sum loss: 0.000000 Sign Loss: 0.000000\n",
      "Epoch [800/1000], MSE Loss: 0.000010 Sum loss: 0.000000 Sign Loss: 0.000000\n",
      "Epoch [900/1000], MSE Loss: 0.000010 Sum loss: 0.000000 Sign Loss: 0.000000\n",
      "Epoch [1000/1000], MSE Loss: 0.000009 Sum loss: 0.000000 Sign Loss: 0.000000\n",
      "Test Mean Squared Error: 0.00000880298011907144\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_data_arrays_in, cleaned_data_arrays_out, test_size=0.2, random_state=42)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class PhysicsInformedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PhysicsInformedNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(X_train.shape[1], 256)\n",
    "        self.hidden2 = nn.Linear(256, 128)\n",
    "        self.hidden3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, y_train_tensor.shape[1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = torch.relu(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = PhysicsInformedNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Custom loss function to enforce the sum of outputs to be zero\n",
    "def custom_loss(output, target):\n",
    "    mse_loss = criterion(output, target)\n",
    "\n",
    "    sign_penalty = 0\n",
    "    for i in range(output.shape[1]):\n",
    "        sign_penalty += torch.mean(torch.relu(-output[:, i] * torch.sign(target[:, i])))\n",
    "    sign_penalty = sign_penalty / output.shape[1]\n",
    "    \n",
    "    sign_loss=sign_penalty**2\n",
    "    sum_constraint = torch.sum(output, dim=1)\n",
    "    sum_loss = torch.mean(sum_constraint ** 2)\n",
    "    return [mse_loss, sum_loss,sign_loss]\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = custom_loss(outputs, y_train_tensor)\n",
    "    loss_sum= loss[0]+loss[1]+1000*loss[2] \n",
    "    loss_sum.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], MSE Loss: {loss[0].item():.6f} Sum loss: {loss[1].item():.6f} Sign Loss: {loss[2].item():.6f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor)\n",
    "    mse = criterion(predictions, y_test_tensor)\n",
    "    print(f'Test Mean Squared Error: {mse.item():.20f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in=cleaned_data_arrays_in\n",
    "ds_out=cleaned_data_arrays_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.3028e-04,  1.2299e-03,  5.0677e-04,  6.3368e-04,  3.0687e-04,\n",
      "         2.8238e-04,  1.7975e-05, -3.2791e-04, -6.0621e-04, -6.1639e-04,\n",
      "        -1.3885e-03, -4.0108e-04], grad_fn=<ViewBackward0>)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "[9.86153960e-01 7.37698516e-03 6.60287275e-04 9.57532786e-04\n",
      " 1.49875123e-03 2.31453776e-03 3.02756933e-04 4.31457993e-05\n",
      " 9.21014216e-05 1.35843962e-04 2.43077782e-04 2.21020615e-04\n",
      " 3.55377293e-02]\n",
      "[ 1.62720680e-05  4.52380627e-05  9.28282971e-05  1.66275189e-04\n",
      "  2.75537139e-04  4.32153698e-04 -2.93110381e-04 -4.31457993e-05\n",
      " -9.21014216e-05 -1.35843962e-04 -2.43077782e-04 -2.21020615e-04]\n"
     ]
    }
   ],
   "source": [
    "k=25009\n",
    "print(model(torch.tensor(ds_in[k], dtype=torch.float32)))\n",
    "print(sum(model(torch.tensor(ds_in[k], dtype=torch.float32))))\n",
    "print(ds_in[k])\n",
    "print(ds_out[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of incorrect different negative signs: 202102 As a %: 5.708689663899632\n"
     ]
    }
   ],
   "source": [
    "negative_sign_differences = 0\n",
    "#ds_in_combine, ds_out_combine\n",
    "for i in range(len(ds_in)):\n",
    "    predicted_output = model(torch.tensor(ds_in[i], dtype=torch.float32))\n",
    "    actual_output = torch.tensor(ds_out[i])\n",
    "    \n",
    "    # Compare the signs of the predicted and actual outputs\n",
    "    predicted_signs = torch.sign(predicted_output)\n",
    "    actual_signs = torch.sign(actual_output)\n",
    "    \n",
    "    # Count the number of different signs\n",
    "    negative_sign_differences += torch.sum(predicted_signs != actual_signs).item()\n",
    "\n",
    "print(f'Total number of incorrect different negative signs: {negative_sign_differences} As a %: {100*negative_sign_differences/(len(ds_in)*len(ds_out[0]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Orders of Magnitude Difference: 0.912179\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    avg_orders_of_magnitude_diff = calculate_orders_of_magnitude_difference(test_predictions, y_test_tensor)\n",
    "    print(f'Average Orders of Magnitude Difference: {avg_orders_of_magnitude_diff.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average output for model run 295020: 0.00003892220030844522\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average of each model run\n",
    "averages = 0\n",
    "\n",
    "for i in range(len(ds_in)):\n",
    "    predicted_output = model(torch.tensor(ds_in[i], dtype=torch.float32))\n",
    "    average_output = torch.sum(predicted_output).item()\n",
    "    averages+=average_output\n",
    "\n",
    "# Print the averages\n",
    "averages=averages/len(ds_in)\n",
    "print(f\"Average output for model run {i}: {averages:.20f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to CSV files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract weights from the model\n",
    "hidden1_weights = model.hidden1.weight.detach().numpy()\n",
    "hidden2_weights = model.hidden2.weight.detach().numpy()\n",
    "hidden3_weights = model.hidden3.weight.detach().numpy()\n",
    "\n",
    "# Create a DataFrame for each layer's weights\n",
    "df_hidden1 = pd.DataFrame(hidden1_weights)\n",
    "df_hidden2 = pd.DataFrame(hidden2_weights)\n",
    "df_hidden3 = pd.DataFrame(hidden3_weights)\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "df_hidden1.to_csv('hidden1_weights.csv', index=False, header=False)\n",
    "df_hidden2.to_csv('hidden2_weights.csv', index=False, header=False)\n",
    "df_hidden3.to_csv('hidden3_weights.csv', index=False, header=False)\n",
    "\n",
    "print(\"Model weights saved to CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131674, 12)\n",
      "(131674, 13)\n"
     ]
    }
   ],
   "source": [
    "#Let's just try to classify sign\n",
    "cleaned_data_arrays_out_sign=[]\n",
    "for i in range(len(cleaned_data_arrays_out)):\n",
    "    cleaned_data_arrays_out_sign.append(np.sign(cleaned_data_arrays_out[i]))\n",
    "cleaned_data_arrays_out_sign=np.array(cleaned_data_arrays_out_sign)\n",
    "print(cleaned_data_arrays_out_sign.shape)\n",
    "print(cleaned_data_arrays_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor shape: torch.Size([105339, 13])\n",
      "y_train_tensor shape: torch.Size([105339, 12])\n",
      "Epoch [1/10], Loss: 0.6973\n",
      "Epoch [2/10], Loss: 0.6917\n",
      "Epoch [3/10], Loss: 0.6860\n",
      "Epoch [4/10], Loss: 0.6798\n",
      "Epoch [5/10], Loss: 0.6731\n",
      "Epoch [6/10], Loss: 0.6658\n",
      "Epoch [7/10], Loss: 0.6580\n",
      "Epoch [8/10], Loss: 0.6491\n",
      "Epoch [9/10], Loss: 0.6394\n",
      "Epoch [10/10], Loss: 0.6285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_data_arrays_in, cleaned_data_arrays_out_sign, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Use `long` for integer labels\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(f\"X_train_tensor shape: {X_train_tensor.shape}\")\n",
    "print(f\"y_train_tensor shape: {y_train_tensor.shape}\")\n",
    "# Define the model\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_test.shape[1], 256)  # First layer with 64 units\n",
    "        self.fc2 = nn.Linear(256, 128)  # Second layer with 32 units\n",
    "        self.fc3 = nn.Linear(128, 64)  # Output layer\n",
    "        self.fc4 = nn.Linear(64, y_test.shape[1],)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Define input and output sizes based on the tensor shapes\n",
    "input_size = 13  # X has 13 features\n",
    "output_size = 12  # y has 12 labels\n",
    "\n",
    "# Instantiate the model, define loss and optimizer\n",
    "model = BinaryClassifier(input_size, output_size)\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy for multi-label binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert y_train_tensor to range [0, 1] (optional)\n",
    "y_train_tensor = (y_train_tensor + 1) / 2\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Convert outputs back to -1 or 1 after prediction if needed\n",
    "final_outputs = (outputs * 2) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in=cleaned_data_arrays_in\n",
    "ds_out=cleaned_data_arrays_out_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of different negative signs: 382523 As a %: 22.346739792097267\n"
     ]
    }
   ],
   "source": [
    "negative_sign_differences = 0\n",
    "#ds_in_combine, ds_out_combine\n",
    "for i in range(len(ds_in)):\n",
    "    predicted_output = model(torch.tensor(ds_in[i], dtype=torch.float32))\n",
    "    actual_output = torch.tensor(ds_out[i])\n",
    "    \n",
    "    # Compare the signs of the predicted and actual outputs\n",
    "    predicted_signs = torch.sign(predicted_output)\n",
    "    actual_signs = torch.sign(actual_output)\n",
    "    \n",
    "    # Count the number of different signs\n",
    "    negative_sign_differences += torch.sum(predicted_signs != actual_signs).item()\n",
    "\n",
    "print(f'Total number of different negative signs: {negative_sign_differences} As a %: {100*negative_sign_differences/(len(ds_in)*len(ds_in[0]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
