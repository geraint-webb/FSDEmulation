{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "import h5netcdf\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as sig\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../Data/ds_in.csv', '../Data/ds_inBering.csv', '../Data/ds_inBering30day10.csv', '../Data/ds_inBering30day20.csv', '../Data/ds_inBering30day23.csv', '../Data/ds_inBering30day26.csv', '../Data/ds_inBering30day29.csv', '../Data/ds_inBering30day31.csv', '../Data/ds_inBering30day41.csv', '../Data/ds_inBering30day46.csv', '../Data/ds_inBering30day48.csv', '../Data/ds_inBering30day51.csv', '../Data/ds_inBering30day60.csv', '../Data/ds_inEastSH30day05.csv', '../Data/ds_inEastSH30day10.csv', '../Data/ds_inSH.csv', '../Data/ds_inSH30day10.csv', '../Data/ds_inSH30day20.csv', '../Data/ds_inSHWeddell30day10.csv', '../Data/ds_inSHWeddell30day20.csv', '../Data/ds_inWeddell.csv', '../Data/ds_in_30LeftFranday05.csv', '../Data/ds_in_30LeftFranday10.csv', '../Data/ds_in_30LeftFranday20.csv', '../Data/ds_in_30LeftFranday25.csv', '../Data/ds_in_30LeftFranday27.csv', '../Data/ds_in_30LeftFranday28.csv', '../Data/ds_in_30LeftFranday60.csv', '../Data/ds_in_30RightFranday05.csv', '../Data/ds_in_30RightFranday10.csv', '../Data/ds_in_30RightFranday15.csv', '../Data/ds_in_30RightFranday20.csv', '../Data/ds_in_30RightFranday25.csv', '../Data/ds_in_30RightFranday30.csv', '../Data/ds_in_30RightFranday31.csv', '../Data/ds_in_30RightFranday41.csv', '../Data/ds_in_30RightFranday46.csv', '../Data/ds_in_30RightFranday51.csv', '../Data/ds_in_30RightFranday60.csv', '../Data/ds_in_30RightFranday70.csv', '../Data/ds_in_30day10.csv', '../Data/ds_in_30day20.csv', '../Data/ds_in_30day23.csv', '../Data/ds_in_30day26.csv', '../Data/ds_in_30day29.csv', '../Data/ds_in_30day31.csv', '../Data/ds_in_30day41.csv', '../Data/ds_in_30day46.csv', '../Data/ds_in_30day48.csv', '../Data/ds_in_30day51.csv', '../Data/ds_in_30day60.csv']\n",
      "['../Data/ds_out.csv', '../Data/ds_outBering.csv', '../Data/ds_outBering30day10.csv', '../Data/ds_outBering30day20.csv', '../Data/ds_outBering30day23.csv', '../Data/ds_outBering30day26.csv', '../Data/ds_outBering30day29.csv', '../Data/ds_outBering30day31.csv', '../Data/ds_outBering30day41.csv', '../Data/ds_outBering30day46.csv', '../Data/ds_outBering30day48.csv', '../Data/ds_outBering30day51.csv', '../Data/ds_outBering30day60.csv', '../Data/ds_outEastSH30day05.csv', '../Data/ds_outEastSH30day10.csv', '../Data/ds_outSH.csv', '../Data/ds_outSH30day10.csv', '../Data/ds_outSH30day20.csv', '../Data/ds_outSHWedell30day10.csv', '../Data/ds_outSHWedell30day20.csv', '../Data/ds_outWeddell.csv', '../Data/ds_out_30LeftFranday05.csv', '../Data/ds_out_30LeftFranday10.csv', '../Data/ds_out_30LeftFranday20.csv', '../Data/ds_out_30LeftFranday25.csv', '../Data/ds_out_30LeftFranday27.csv', '../Data/ds_out_30LeftFranday28.csv', '../Data/ds_out_30LeftFranday60.csv', '../Data/ds_out_30RightFranday05.csv', '../Data/ds_out_30RightFranday10.csv', '../Data/ds_out_30RightFranday15.csv', '../Data/ds_out_30RightFranday20.csv', '../Data/ds_out_30RightFranday25.csv', '../Data/ds_out_30RightFranday30.csv', '../Data/ds_out_30RightFranday31.csv', '../Data/ds_out_30RightFranday41.csv', '../Data/ds_out_30RightFranday46.csv', '../Data/ds_out_30RightFranday51.csv', '../Data/ds_out_30RightFranday60.csv', '../Data/ds_out_30RightFranday70.csv', '../Data/ds_out_30day10.csv', '../Data/ds_out_30day20.csv', '../Data/ds_out_30day23.csv', '../Data/ds_out_30day26.csv', '../Data/ds_out_30day29.csv', '../Data/ds_out_30day31.csv', '../Data/ds_out_30day41.csv', '../Data/ds_out_30day46.csv', '../Data/ds_out_30day48.csv', '../Data/ds_out_30day51.csv', '../Data/ds_out_30day60.csv']\n"
     ]
    }
   ],
   "source": [
    "file_path='../Data/'\n",
    "data_list_in=glob.glob(file_path+'ds_in*')\n",
    "data_list_out=glob.glob(file_path+'ds_out*')\n",
    "data_list_in.sort()\n",
    "data_list_out.sort()\n",
    "print(data_list_in)\n",
    "print(data_list_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/geraint/tmp/ipykernel_9844/1827004139.py:1: UserWarning: loadtxt: input contained no data: \"../Data/ds_inEastSH30day10.csv\"\n",
      "  data_arrays_in = [np.loadtxt(file, delimiter=',') for file in data_list_in]\n",
      "/glade/derecho/scratch/geraint/tmp/ipykernel_9844/1827004139.py:2: UserWarning: loadtxt: input contained no data: \"../Data/ds_outEastSH30day10.csv\"\n",
      "  data_arrays_out = [np.loadtxt(file, delimiter=',') for file in data_list_out]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_arrays_in[0]: (45693, 13)\n",
      "Shape of data_arrays_in[1]: (41227, 13)\n",
      "Shape of data_arrays_in[2]: (30775, 13)\n",
      "Shape of data_arrays_in[3]: (56466, 13)\n",
      "Shape of data_arrays_in[4]: (95697, 13)\n",
      "Shape of data_arrays_in[5]: (35113, 13)\n",
      "Shape of data_arrays_in[6]: (31230, 13)\n",
      "Shape of data_arrays_in[7]: (6052, 13)\n",
      "Shape of data_arrays_in[8]: (17587, 13)\n",
      "Shape of data_arrays_in[9]: (99267, 13)\n",
      "Shape of data_arrays_in[10]: (19625, 13)\n",
      "Shape of data_arrays_in[11]: (18723, 13)\n",
      "Shape of data_arrays_in[12]: (123974, 13)\n",
      "Shape of data_arrays_in[13]: (2029, 13)\n",
      "Shape of data_arrays_in[14]: (0,)\n",
      "Shape of data_arrays_in[15]: (44050, 13)\n",
      "Shape of data_arrays_in[16]: (41869, 13)\n",
      "Shape of data_arrays_in[17]: (3, 13)\n",
      "Shape of data_arrays_in[18]: (16273, 13)\n",
      "Shape of data_arrays_in[19]: (371, 13)\n",
      "Shape of data_arrays_in[20]: (15356, 13)\n",
      "Shape of data_arrays_in[21]: (50594, 13)\n",
      "Shape of data_arrays_in[22]: (68024, 13)\n",
      "Shape of data_arrays_in[23]: (19329, 13)\n",
      "Shape of data_arrays_in[24]: (71206, 13)\n",
      "Shape of data_arrays_in[25]: (22584, 13)\n",
      "Shape of data_arrays_in[26]: (43325, 13)\n",
      "Shape of data_arrays_in[27]: (24432, 13)\n",
      "Shape of data_arrays_in[28]: (11712, 13)\n",
      "Shape of data_arrays_in[29]: (19901, 13)\n",
      "Shape of data_arrays_in[30]: (16889, 13)\n",
      "Shape of data_arrays_in[31]: (9925, 13)\n",
      "Shape of data_arrays_in[32]: (21228, 13)\n",
      "Shape of data_arrays_in[33]: (22037, 13)\n",
      "Shape of data_arrays_in[34]: (14577, 13)\n",
      "Shape of data_arrays_in[35]: (49333, 13)\n",
      "Shape of data_arrays_in[36]: (12735, 13)\n",
      "Shape of data_arrays_in[37]: (13998, 13)\n",
      "Shape of data_arrays_in[38]: (6357, 13)\n",
      "Shape of data_arrays_in[39]: (8619, 13)\n",
      "Shape of data_arrays_in[40]: (86622, 13)\n",
      "Shape of data_arrays_in[41]: (71015, 13)\n",
      "Shape of data_arrays_in[42]: (27149, 13)\n",
      "Shape of data_arrays_in[43]: (34371, 13)\n",
      "Shape of data_arrays_in[44]: (44704, 13)\n",
      "Shape of data_arrays_in[45]: (13557, 13)\n",
      "Shape of data_arrays_in[46]: (31142, 13)\n",
      "Shape of data_arrays_in[47]: (37736, 13)\n",
      "Shape of data_arrays_in[48]: (18501, 13)\n",
      "Shape of data_arrays_in[49]: (37682, 13)\n",
      "Shape of data_arrays_in[50]: (77214, 13)\n",
      "Shape of data_arrays_out[0]: (45693, 12)\n",
      "Shape of data_arrays_out[1]: (41227, 12)\n",
      "Shape of data_arrays_out[2]: (30775, 12)\n",
      "Shape of data_arrays_out[3]: (56466, 12)\n",
      "Shape of data_arrays_out[4]: (95697, 12)\n",
      "Shape of data_arrays_out[5]: (35113, 12)\n",
      "Shape of data_arrays_out[6]: (31230, 12)\n",
      "Shape of data_arrays_out[7]: (6052, 12)\n",
      "Shape of data_arrays_out[8]: (17587, 12)\n",
      "Shape of data_arrays_out[9]: (99267, 12)\n",
      "Shape of data_arrays_out[10]: (19625, 12)\n",
      "Shape of data_arrays_out[11]: (18723, 12)\n",
      "Shape of data_arrays_out[12]: (123974, 12)\n",
      "Shape of data_arrays_out[13]: (2029, 12)\n",
      "Shape of data_arrays_out[14]: (0,)\n",
      "Shape of data_arrays_out[15]: (44050, 12)\n",
      "Shape of data_arrays_out[16]: (41869, 12)\n",
      "Shape of data_arrays_out[17]: (3, 12)\n",
      "Shape of data_arrays_out[18]: (16273, 12)\n",
      "Shape of data_arrays_out[19]: (371, 12)\n",
      "Shape of data_arrays_out[20]: (15356, 12)\n",
      "Shape of data_arrays_out[21]: (50594, 12)\n",
      "Shape of data_arrays_out[22]: (68024, 12)\n",
      "Shape of data_arrays_out[23]: (19329, 12)\n",
      "Shape of data_arrays_out[24]: (71206, 12)\n",
      "Shape of data_arrays_out[25]: (22584, 12)\n",
      "Shape of data_arrays_out[26]: (43325, 12)\n",
      "Shape of data_arrays_out[27]: (24432, 12)\n",
      "Shape of data_arrays_out[28]: (11712, 12)\n",
      "Shape of data_arrays_out[29]: (19901, 12)\n",
      "Shape of data_arrays_out[30]: (16889, 12)\n",
      "Shape of data_arrays_out[31]: (9925, 12)\n",
      "Shape of data_arrays_out[32]: (21228, 12)\n",
      "Shape of data_arrays_out[33]: (22037, 12)\n",
      "Shape of data_arrays_out[34]: (14577, 12)\n",
      "Shape of data_arrays_out[35]: (49333, 12)\n",
      "Shape of data_arrays_out[36]: (12735, 12)\n",
      "Shape of data_arrays_out[37]: (13998, 12)\n",
      "Shape of data_arrays_out[38]: (6357, 12)\n",
      "Shape of data_arrays_out[39]: (8619, 12)\n",
      "Shape of data_arrays_out[40]: (86622, 12)\n",
      "Shape of data_arrays_out[41]: (71015, 12)\n",
      "Shape of data_arrays_out[42]: (27149, 12)\n",
      "Shape of data_arrays_out[43]: (34371, 12)\n",
      "Shape of data_arrays_out[44]: (44704, 12)\n",
      "Shape of data_arrays_out[45]: (13557, 12)\n",
      "Shape of data_arrays_out[46]: (31142, 12)\n",
      "Shape of data_arrays_out[47]: (37736, 12)\n",
      "Shape of data_arrays_out[48]: (18501, 12)\n",
      "Shape of data_arrays_out[49]: (37682, 12)\n",
      "Shape of data_arrays_out[50]: (77214, 12)\n"
     ]
    }
   ],
   "source": [
    "data_arrays_in = [np.loadtxt(file, delimiter=',') for file in data_list_in]\n",
    "data_arrays_out = [np.loadtxt(file, delimiter=',') for file in data_list_out]\n",
    "\n",
    "# Print the shapes of the arrays to verify\n",
    "for i, array in enumerate(data_arrays_in):\n",
    "    print(f\"Shape of data_arrays_in[{i}]: {array.shape}\")\n",
    "\n",
    "for i, array in enumerate(data_arrays_out):\n",
    "    print(f\"Shape of data_arrays_out[{i}]: {array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of reshaped_data_arrays_in: (1727878, 13)\n"
     ]
    }
   ],
   "source": [
    "# Reshape each array in data_arrays_in to the specified shape and concatenate them\n",
    "reshaped_data_arrays_in = np.concatenate([array.reshape(-1, 13) for array in data_arrays_in])\n",
    "\n",
    "# Print the shape of the reshaped array to verify\n",
    "print(f\"Shape of reshaped_data_arrays_in: {reshaped_data_arrays_in.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of reshaped_data_arrays_out: (1727878, 12)\n"
     ]
    }
   ],
   "source": [
    "# Reshape each array in data_arrays_out to the specified shape and concatenate them\n",
    "reshaped_data_arrays_out = np.concatenate([array.reshape(-1, 12) for array in data_arrays_out])\n",
    "\n",
    "# Print the shape of the reshaped array to verify\n",
    "print(f\"Shape of reshaped_data_arrays_out: {reshaped_data_arrays_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cleaned_data_arrays_in: (1688479, 13)\n",
      "Shape of cleaned_data_arrays_out: (1688479, 12)\n"
     ]
    }
   ],
   "source": [
    "# Clean data_arrays_in and data_arrays_out\n",
    "cleaned_data_arrays_in = []\n",
    "cleaned_data_arrays_out = []\n",
    "\n",
    "for i in range(reshaped_data_arrays_in.shape[0]):\n",
    "        if not np.sum(reshaped_data_arrays_out[i])==0:\n",
    "            cleaned_data_arrays_in.append(reshaped_data_arrays_in[i])\n",
    "            cleaned_data_arrays_out.append(reshaped_data_arrays_out[i])\n",
    "\n",
    "# Convert lists back to numpy arrays if needed\n",
    "cleaned_data_arrays_in = np.array(cleaned_data_arrays_in)\n",
    "cleaned_data_arrays_out = np.array(cleaned_data_arrays_out)\n",
    "\n",
    "# Print the shapes of the cleaned arrays to verify\n",
    "print(f\"Shape of cleaned_data_arrays_in: {cleaned_data_arrays_in.shape}\")\n",
    "print(f\"Shape of cleaned_data_arrays_out: {cleaned_data_arrays_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does cleaned_data_arrays_out have any zero values? True\n",
      "Indices of zero entries in cleaned_data_arrays_out: [[   3226       0]\n",
      " [   3227       0]\n",
      " [   3228       0]\n",
      " ...\n",
      " [1688360       0]\n",
      " [1688360       1]\n",
      " [1688360       2]]\n"
     ]
    }
   ],
   "source": [
    "# Check if cleaned_data_arrays_out has any zero values\n",
    "has_zero_values = np.any(cleaned_data_arrays_out == 0)\n",
    "\n",
    "print(f\"Does cleaned_data_arrays_out have any zero values? {has_zero_values}\")\n",
    "\n",
    "# Find the indices of zero entries in cleaned_data_arrays_out\n",
    "zero_indices = np.argwhere(cleaned_data_arrays_out == 0)\n",
    "\n",
    "print(f\"Indices of zero entries in cleaned_data_arrays_out: {zero_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.637978807091713e-12\n"
     ]
    }
   ],
   "source": [
    "min_non_zero_value = np.min(np.abs(cleaned_data_arrays_out[np.nonzero(cleaned_data_arrays_out)]))\n",
    "print(min_non_zero_value)\n",
    "# So we add 1e-15 to all the zero entries in cleaned_data_arrays_out\n",
    "cleaned_data_arrays_out += 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_data_arrays_in_log=np.log10(np.abs(cleaned_data_arrays_in))\n",
    "cleaned_data_arrays_out_sign=np.sign(cleaned_data_arrays_out)\n",
    "cleaned_data_arrays_out_log=np.abs(np.log10(np.abs(cleaned_data_arrays_out)))\n",
    "cleaned_data_arrays_out_log=cleaned_data_arrays_out_log*cleaned_data_arrays_out_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.30044061  4.82307707  4.48790337  4.21597172  3.98030505  3.77030576\n",
      "  3.58116525  3.41049965  3.25705517  3.12014217  2.99936583 -2.47473637]\n",
      "[ 5.00679016e-06  1.50287524e-05  3.25159635e-05  6.08174596e-05\n",
      "  1.04639330e-04  1.69704843e-04  2.62322021e-04  3.88597808e-04\n",
      "  5.53279824e-04  7.58329290e-04  1.00146129e-03 -3.35168839e-03]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_data_arrays_out_log[1000000])\n",
    "print(cleaned_data_arrays_out[1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168848, 12)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make cleaned_data_arrays_out_log a tenth of the size\n",
    "cleaned_data_arrays_out_log_10=cleaned_data_arrays_out_log[::10]\n",
    "cleaned_data_arrays_in_log_10=cleaned_data_arrays_out_log[::10]\n",
    "cleaned_data_arrays_out_log_10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the orders of magnitude difference\n",
    "def calculate_orders_of_magnitude_difference(predictions, actuals):\n",
    "    differences = torch.abs(predictions - actuals)\n",
    "    orders_of_magnitude_diff = torch.log10(differences + 1e-10) - torch.log10(torch.abs(actuals) + 1e-10)\n",
    "    return torch.mean(torch.abs(orders_of_magnitude_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], MSE Loss: 3.722499 Sum loss: 2.293175 Sign Loss: 0.001909\n",
      "Epoch [200/1000], MSE Loss: 3.271248 Sum loss: 2.108033 Sign Loss: 0.000200\n",
      "Epoch [300/1000], MSE Loss: 3.122973 Sum loss: 2.110924 Sign Loss: 0.000053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 54\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m custom_loss(outputs, y_train_tensor)\n\u001b[1;32m     56\u001b[0m loss_sum\u001b[38;5;241m=\u001b[39m loss[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m0.1\u001b[39m\u001b[38;5;241m*\u001b[39mloss[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;66;03m#+500*loss[2] \u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/my_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/my_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[64], line 25\u001b[0m, in \u001b[0;36mPhysicsInformedNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden1(x))\n\u001b[0;32m---> 25\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden3(x))\n\u001b[1;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n",
      "File \u001b[0;32m~/.conda/envs/my_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/my_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/my_env/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_data_arrays_in_log_10, cleaned_data_arrays_out_log_10, test_size=0.2, random_state=42)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class PhysicsInformedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PhysicsInformedNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(X_train.shape[1], 64)\n",
    "        self.hidden2 = nn.Linear(64, 64)\n",
    "        self.hidden3 = nn.Linear(64, 64)\n",
    "        self.output = nn.Linear(64, y_train_tensor.shape[1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = torch.relu(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = PhysicsInformedNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Custom loss function to enforce the sum of outputs to be zero\n",
    "def custom_loss(output, target):\n",
    "    mse_loss = criterion(output, target)\n",
    "\n",
    "    sign_penalty = 0\n",
    "    for i in range(output.shape[1]):\n",
    "        sign_penalty += torch.mean(torch.relu(-output[:, i] * torch.sign(target[:, i])))\n",
    "    sign_penalty = sign_penalty / output.shape[1]\n",
    "    \n",
    "    sign_loss=sign_penalty**2\n",
    "    sum_constraint = torch.sum(output, dim=1)\n",
    "    sum_loss = torch.mean(sum_constraint ** 2)\n",
    "    return [mse_loss, sum_loss,sign_loss]\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = custom_loss(outputs, y_train_tensor)\n",
    "    loss_sum= loss[0]+0.01*loss[1]#+500*loss[2] \n",
    "    loss_sum.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], MSE Loss: {loss[0].item():.6f} Sum loss: {loss[1].item():.6f} Sign Loss: {loss[2].item():.6f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor)\n",
    "    mse = criterion(predictions, y_test_tensor)\n",
    "    print(f'Test Mean Squared Error: {mse.item():.20f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in=cleaned_data_arrays_in_log_10\n",
    "ds_out=cleaned_data_arrays_out_log_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.6293,  6.5585,  6.1153,  5.9348,  5.5327,  5.2596, -6.8299, -6.8221,\n",
      "        -6.8502, -6.1782, -6.3806, -3.5757], grad_fn=<ViewBackward0>)\n",
      "tensor(-0.6064, grad_fn=<AddBackward0>)\n",
      "[ 6.06335189  5.56910131  5.21399603  4.9261397   4.67679143  4.4546046\n",
      " -7.57243077 -7.63459993 -7.47145762 -7.32506601 -7.19600317 -4.11080974]\n",
      "[ 6.06335189  5.56910131  5.21399603  4.9261397   4.67679143  4.4546046\n",
      " -7.57243077 -7.63459993 -7.47145762 -7.32506601 -7.19600317 -4.11080974]\n"
     ]
    }
   ],
   "source": [
    "k=25009\n",
    "print(model(torch.tensor(ds_in[k], dtype=torch.float32)))\n",
    "print(sum(model(torch.tensor(ds_in[k], dtype=torch.float32))))\n",
    "print(ds_in[k])\n",
    "print(ds_out[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of incorrect different negative signs: 79179 As a %: 3.907804652705392\n"
     ]
    }
   ],
   "source": [
    "negative_sign_differences = 0\n",
    "#ds_in_combine, ds_out_combine\n",
    "for i in range(len(ds_in)):\n",
    "    predicted_output = model(torch.tensor(ds_in[i], dtype=torch.float32))\n",
    "    actual_output = torch.tensor(ds_out[i])\n",
    "    \n",
    "    # Compare the signs of the predicted and actual outputs\n",
    "    predicted_signs = torch.sign(predicted_output)\n",
    "    actual_signs = torch.sign(actual_output)\n",
    "    \n",
    "    # Count the number of different signs\n",
    "    negative_sign_differences += torch.sum(predicted_signs != actual_signs).item()\n",
    "\n",
    "print(f'Total number of incorrect different negative signs: {negative_sign_differences} As a %: {100*negative_sign_differences/(len(ds_in)*len(ds_out[0]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Absolute Error: 1.336944\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean absolute error (MAE) across all samples\n",
    "def calculate_mean_absolute_error(predictions, actuals):\n",
    "    absolute_errors = torch.abs(predictions - actuals)\n",
    "    mean_absolute_error = torch.mean(absolute_errors)\n",
    "    return mean_absolute_error\n",
    "\n",
    "# Evaluate the model on the test set and calculate MAE\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    mae = calculate_mean_absolute_error(test_predictions, y_test_tensor)\n",
    "    print(f'Test Mean Absolute Error: {mae.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_orders_of_magnitude_difference' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     test_predictions \u001b[38;5;241m=\u001b[39m model(X_test_tensor)\n\u001b[0;32m----> 5\u001b[0m     avg_orders_of_magnitude_diff \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_orders_of_magnitude_difference\u001b[49m(test_predictions, y_test_tensor)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Orders of Magnitude Difference: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_orders_of_magnitude_diff\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_orders_of_magnitude_difference' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    avg_orders_of_magnitude_diff = calculate_orders_of_magnitude_difference(test_predictions, y_test_tensor)\n",
    "    print(f'Average Orders of Magnitude Difference: {avg_orders_of_magnitude_diff.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average output for model run 168847: 0.06786338880884898450\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average of each model run\n",
    "averages = 0\n",
    "\n",
    "for i in range(len(ds_in)):\n",
    "    predicted_output = model(torch.tensor(ds_in[i], dtype=torch.float32))\n",
    "    average_output = torch.sum(predicted_output).item()\n",
    "    averages+=average_output\n",
    "\n",
    "# Print the averages\n",
    "averages=averages/(len(ds_in)*len(ds_out[0]))\n",
    "print(f\"Average output for model run {i}: {averages:.20f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.764627456665039"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_output = model(torch.tensor(ds_in[2505], dtype=torch.float32))\n",
    "average_output = torch.sum(predicted_output).item()\n",
    "average_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to CSV files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract weights from the model\n",
    "hidden1_weights = model.hidden1.weight.detach().numpy()\n",
    "hidden2_weights = model.hidden2.weight.detach().numpy()\n",
    "hidden3_weights = model.hidden3.weight.detach().numpy()\n",
    "\n",
    "# Create a DataFrame for each layer's weights\n",
    "df_hidden1 = pd.DataFrame(hidden1_weights)\n",
    "df_hidden2 = pd.DataFrame(hidden2_weights)\n",
    "df_hidden3 = pd.DataFrame(hidden3_weights)\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "df_hidden1.to_csv('hidden1_weights.csv', index=False, header=False)\n",
    "df_hidden2.to_csv('hidden2_weights.csv', index=False, header=False)\n",
    "df_hidden3.to_csv('hidden3_weights.csv', index=False, header=False)\n",
    "\n",
    "print(\"Model weights saved to CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131674, 12)\n",
      "(131674, 13)\n"
     ]
    }
   ],
   "source": [
    "#Let's just try to classify sign\n",
    "cleaned_data_arrays_out_sign=[]\n",
    "for i in range(len(cleaned_data_arrays_out)):\n",
    "    cleaned_data_arrays_out_sign.append(np.sign(cleaned_data_arrays_out[i]))\n",
    "cleaned_data_arrays_out_sign=np.array(cleaned_data_arrays_out_sign)\n",
    "print(cleaned_data_arrays_out_sign.shape)\n",
    "print(cleaned_data_arrays_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor shape: torch.Size([105339, 13])\n",
      "y_train_tensor shape: torch.Size([105339, 12])\n",
      "Epoch [1/10], Loss: 0.6973\n",
      "Epoch [2/10], Loss: 0.6917\n",
      "Epoch [3/10], Loss: 0.6860\n",
      "Epoch [4/10], Loss: 0.6798\n",
      "Epoch [5/10], Loss: 0.6731\n",
      "Epoch [6/10], Loss: 0.6658\n",
      "Epoch [7/10], Loss: 0.6580\n",
      "Epoch [8/10], Loss: 0.6491\n",
      "Epoch [9/10], Loss: 0.6394\n",
      "Epoch [10/10], Loss: 0.6285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_data_arrays_in, cleaned_data_arrays_out_sign, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Use `long` for integer labels\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(f\"X_train_tensor shape: {X_train_tensor.shape}\")\n",
    "print(f\"y_train_tensor shape: {y_train_tensor.shape}\")\n",
    "# Define the model\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_test.shape[1], 256)  # First layer with 64 units\n",
    "        self.fc2 = nn.Linear(256, 128)  # Second layer with 32 units\n",
    "        self.fc3 = nn.Linear(128, 64)  # Output layer\n",
    "        self.fc4 = nn.Linear(64, y_test.shape[1],)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Define input and output sizes based on the tensor shapes\n",
    "input_size = 13  # X has 13 features\n",
    "output_size = 12  # y has 12 labels\n",
    "\n",
    "# Instantiate the model, define loss and optimizer\n",
    "model = BinaryClassifier(input_size, output_size)\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy for multi-label binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert y_train_tensor to range [0, 1] (optional)\n",
    "y_train_tensor = (y_train_tensor + 1) / 2\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Convert outputs back to -1 or 1 after prediction if needed\n",
    "final_outputs = (outputs * 2) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in=cleaned_data_arrays_in\n",
    "ds_out=cleaned_data_arrays_out_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of different negative signs: 382523 As a %: 22.346739792097267\n"
     ]
    }
   ],
   "source": [
    "negative_sign_differences = 0\n",
    "#ds_in_combine, ds_out_combine\n",
    "for i in range(len(ds_in)):\n",
    "    predicted_output = model(torch.tensor(ds_in[i], dtype=torch.float32))\n",
    "    actual_output = torch.tensor(ds_out[i])\n",
    "    \n",
    "    # Compare the signs of the predicted and actual outputs\n",
    "    predicted_signs = torch.sign(predicted_output)\n",
    "    actual_signs = torch.sign(actual_output)\n",
    "    \n",
    "    # Count the number of different signs\n",
    "    negative_sign_differences += torch.sum(predicted_signs != actual_signs).item()\n",
    "\n",
    "print(f'Total number of different negative signs: {negative_sign_differences} As a %: {100*negative_sign_differences/(len(ds_in)*len(ds_in[0]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
