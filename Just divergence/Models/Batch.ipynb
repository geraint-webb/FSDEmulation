{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "import h5netcdf\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as sig\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../Data/ds_in.csv', '../Data/ds_inBering.csv', '../Data/ds_inSH.csv', '../Data/ds_inWeddell.csv']\n",
      "['../Data/ds_out.csv', '../Data/ds_outBering.csv', '../Data/ds_outSH.csv', '../Data/ds_outWeddell.csv']\n"
     ]
    }
   ],
   "source": [
    "file_path='../Data/'\n",
    "data_list_in=glob.glob(file_path+'ds_in*')\n",
    "data_list_out=glob.glob(file_path+'ds_out*')\n",
    "data_list_in.sort()\n",
    "data_list_out.sort()\n",
    "print(data_list_in)\n",
    "print(data_list_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_arrays_in[0]: (45693, 13)\n",
      "Shape of data_arrays_in[1]: (41227, 13)\n",
      "Shape of data_arrays_in[2]: (44050, 13)\n",
      "Shape of data_arrays_in[3]: (15356, 13)\n",
      "Shape of data_arrays_out[0]: (45693, 12)\n",
      "Shape of data_arrays_out[1]: (41227, 12)\n",
      "Shape of data_arrays_out[2]: (44050, 12)\n",
      "Shape of data_arrays_out[3]: (15356, 12)\n"
     ]
    }
   ],
   "source": [
    "data_arrays_in = [np.loadtxt(file, delimiter=',') for file in data_list_in]\n",
    "data_arrays_out = [np.loadtxt(file, delimiter=',') for file in data_list_out]\n",
    "\n",
    "# Print the shapes of the arrays to verify\n",
    "for i, array in enumerate(data_arrays_in):\n",
    "    print(f\"Shape of data_arrays_in[{i}]: {array.shape}\")\n",
    "\n",
    "for i, array in enumerate(data_arrays_out):\n",
    "    print(f\"Shape of data_arrays_out[{i}]: {array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of reshaped_data_arrays_in: (146326, 13)\n"
     ]
    }
   ],
   "source": [
    "# Reshape each array in data_arrays_in to the specified shape and concatenate them\n",
    "reshaped_data_arrays_in = np.concatenate([array.reshape(-1, 13) for array in data_arrays_in])\n",
    "\n",
    "# Print the shape of the reshaped array to verify\n",
    "print(f\"Shape of reshaped_data_arrays_in: {reshaped_data_arrays_in.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of reshaped_data_arrays_out: (146326, 12)\n"
     ]
    }
   ],
   "source": [
    "# Reshape each array in data_arrays_out to the specified shape and concatenate them\n",
    "reshaped_data_arrays_out = np.concatenate([array.reshape(-1, 12) for array in data_arrays_out])\n",
    "\n",
    "# Print the shape of the reshaped array to verify\n",
    "print(f\"Shape of reshaped_data_arrays_out: {reshaped_data_arrays_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cleaned_data_arrays_in: (131674, 13)\n",
      "Shape of cleaned_data_arrays_out: (131674, 12)\n"
     ]
    }
   ],
   "source": [
    "# Clean data_arrays_in and data_arrays_out\n",
    "cleaned_data_arrays_in = []\n",
    "cleaned_data_arrays_out = []\n",
    "\n",
    "for i in range(reshaped_data_arrays_in.shape[0]):\n",
    "        if not np.sum(reshaped_data_arrays_out[i])==0:\n",
    "            cleaned_data_arrays_in.append(reshaped_data_arrays_in[i])\n",
    "            cleaned_data_arrays_out.append(reshaped_data_arrays_out[i])\n",
    "\n",
    "# Convert lists back to numpy arrays if needed\n",
    "cleaned_data_arrays_in = np.array(cleaned_data_arrays_in)\n",
    "cleaned_data_arrays_out = np.array(cleaned_data_arrays_out)\n",
    "\n",
    "# Print the shapes of the cleaned arrays to verify\n",
    "print(f\"Shape of cleaned_data_arrays_in: {cleaned_data_arrays_in.shape}\")\n",
    "print(f\"Shape of cleaned_data_arrays_out: {cleaned_data_arrays_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the orders of magnitude difference\n",
    "def calculate_orders_of_magnitude_difference(predictions, actuals):\n",
    "    differences = torch.abs(predictions - actuals)\n",
    "    orders_of_magnitude_diff = torch.log10(differences + 1e-10) - torch.log10(torch.abs(actuals) + 1e-10)\n",
    "    return torch.mean(torch.abs(orders_of_magnitude_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 19:11:59.234682: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-26 19:11:59.384010: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "mean_squared_error() missing 2 required positional arguments: 'y_true' and 'y_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Initialize the model, loss function, and optimizer\u001b[39;00m\n\u001b[1;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m PhysicsInformedNN()\n\u001b[0;32m---> 27\u001b[0m criterion \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSE\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Custom loss function to enforce the sum of outputs to be zero\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: mean_squared_error() missing 2 required positional arguments: 'y_true' and 'y_pred'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert data to TensorFlow tensors\n",
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_data_arrays_in, cleaned_data_arrays_out, test_size=0.2, random_state=42)\n",
    "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class PhysicsInformedNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(PhysicsInformedNN, self).__init__()\n",
    "        self.hidden1 = tf.keras.layers.Dense(100, activation='relu')\n",
    "        self.hidden2 = tf.keras.layers.Dense(100, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(y_train_tensor.shape[1])\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = PhysicsInformedNN()\n",
    "criterion = tf.keras.losses.MSE()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Custom loss function to enforce the sum of outputs to be zero\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mse_loss = criterion(y_true, y_pred)\n",
    "\n",
    "    sign_penalty = 0\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        sign_penalty += tf.reduce_mean(tf.nn.relu(-y_pred[:, i] * tf.sign(y_true[:, i])))\n",
    "    sign_penalty = sign_penalty / y_pred.shape[1]\n",
    "    \n",
    "    sign_loss = sign_penalty ** 2\n",
    "    sum_constraint = tf.reduce_sum(y_pred, axis=1)\n",
    "    sum_loss = tf.reduce_mean(sum_constraint ** 2)\n",
    "    return mse_loss + sum_loss + 100000 * sign_loss\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "model.compile(optimizer=optimizer, loss=custom_loss)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "history = model.fit(X_train_tensor, y_train_tensor, epochs=num_epochs, batch_size=32, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = model.evaluate(X_test_tensor, y_test_tensor, verbose=0)\n",
    "print(f'Test Mean Squared Error: {mse:.20f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=25000\n",
    "print(model(torch.tensor(ds_in[k], dtype=torch.float32)))\n",
    "print(sum(model(torch.tensor(ds_in[k], dtype=torch.float32))))\n",
    "print(ds_in[k])\n",
    "print(ds_out[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sign_differences = 0\n",
    "#ds_in_combine, ds_out_combine\n",
    "for i in range(len(ds_in_combine)):\n",
    "    predicted_output = model(torch.tensor(ds_in_combine[i], dtype=torch.float32))\n",
    "    actual_output = torch.tensor(ds_out_combine[i])\n",
    "    \n",
    "    # Compare the signs of the predicted and actual outputs\n",
    "    predicted_signs = torch.sign(predicted_output)\n",
    "    actual_signs = torch.sign(actual_output)\n",
    "    \n",
    "    # Count the number of different signs\n",
    "    negative_sign_differences += torch.sum(predicted_signs != actual_signs).item()\n",
    "\n",
    "print(f'Total number of different negative signs: {negative_sign_differences} As a %: {100*negative_sign_differences/(len(ds_in_combine)*len(ds_in_combine[0]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    avg_orders_of_magnitude_diff = calculate_orders_of_magnitude_difference(test_predictions, y_test_tensor)\n",
    "    print(f'Average Orders of Magnitude Difference: {avg_orders_of_magnitude_diff.item():.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
